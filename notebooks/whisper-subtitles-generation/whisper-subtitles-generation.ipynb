{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvj1MLX408FE"
      },
      "source": [
        "# Video Subtitle Generation using Whisper and OpenVINO™\n",
        "\n",
        "[Whisper](https://openai.com/blog/whisper/) is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web. It is a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "![asr-training-data-desktop.svg](https://user-images.githubusercontent.com/29454499/204536347-28976978-9a07-416c-acff-fc1214bbfbe0.svg)\n",
        "\n",
        "You can find more information about this model in the [research paper](https://cdn.openai.com/papers/whisper.pdf), [OpenAI blog](https://openai.com/blog/whisper/), [model card](https://github.com/openai/whisper/blob/main/model-card.md) and GitHub [repository](https://github.com/openai/whisper).\n",
        "\n",
        "In this notebook, we will use Whisper model with [OpenVINO Generate API](https://github.com/openvinotoolkit/openvino.genai) for [Whisper automatic speech recognition scenarios](https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md) to generate subtitles in a sample video. Additionally, we will use [NNCF](https://github.com/openvinotoolkit/nncf) improving model performance by INT8 quantization.\n",
        "Notebook contains the following steps:\n",
        "1. Download the model.\n",
        "2. Instantiate the PyTorch model pipeline.\n",
        "3. Convert model to OpenVINO IR, using model conversion API.\n",
        "4. Run the Whisper pipeline with OpenVINO models.\n",
        "5. Quantize the OpenVINO model with NNCF.\n",
        "6. Check quantized model result for the demo video.\n",
        "7. Compare model size, performance and accuracy of FP32 and quantized INT8 models.\n",
        "8. Launch Interactive demo for video subtitles generation.\n",
        "\n",
        "\n",
        "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/whisper-subtitles-generation/whisper-subtitles-generation.ipynb\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXk6d54O08FG"
      },
      "source": [
        "#### Table of contents:\n",
        "\n",
        "- [Prerequisites](#Prerequisites)\n",
        "- [Instantiate model](#Instantiate-model)\n",
        "    - [Convert model to OpenVINO Intermediate Representation (IR) format.](#Convert-model-to-OpenVINO-Intermediate-Representation-(IR)-format.)\n",
        "- [Prepare inference pipeline](#Prepare-inference-pipeline)\n",
        "    - [Select inference device](#Select-inference-device)\n",
        "- [Run video transcription pipeline](#Run-video-transcription-pipeline)\n",
        "- [Quantization](#Quantization)\n",
        "    - [Prepare calibration datasets](#Prepare-calibration-datasets)\n",
        "    - [Quantize Whisper encoder and decoder models](#Quantize-Whisper-encoder-and-decoder-models)\n",
        "    - [Run quantized model inference](#Run-quantized-model-inference)\n",
        "    - [Compare performance and accuracy of the original and quantized models](#Compare-performance-and-accuracy-of-the-original-and-quantized-models)\n",
        "- [Interactive demo](#Interactive-demo)\n",
        "\n",
        "\n",
        "### Installation Instructions\n",
        "\n",
        "This is a self-contained example that relies solely on its own code.\n",
        "\n",
        "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
        "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TIqaf8k08FG"
      },
      "source": [
        "## Prerequisites\n",
        "[back to top ⬆️](#Table-of-contents:)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw6hk4Zv08FH"
      },
      "source": [
        "Install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFknhG_v08FH",
        "outputId": "b771256f-fe95-4bad-fd85-06e3364b0dd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.1/249.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for optimum-intel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "import importlib.metadata\n",
        "import importlib.util\n",
        "\n",
        "%pip install -q -U \"nncf>=2.15.0\"\n",
        "%pip install -q -U --pre \"openvino>=2025.0\" \"openvino-tokenizers>=2025.0\" \"openvino-genai>=2025.0\" --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
        "%pip install -q -U \"python-ffmpeg<=1.0.16\" \"ffmpeg\" \"moviepy\" \"transformers>=4.45\" \"git+https://github.com/huggingface/optimum-intel.git\" \"torch>=2.1\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
        "%pip install -q -U \"yt_dlp>=2024.8.6\" soundfile librosa jiwer packaging\n",
        "%pip install -q  -U \"gradio>=4.19\" \"typing_extensions>=4.9\"\n",
        "\n",
        "if platform.system() == \"Darwin\":\n",
        "    %pip install -q \"numpy<2.0\"\n",
        "\n",
        "\n",
        "from packaging import version\n",
        "\n",
        "try:\n",
        "    if (\n",
        "        importlib.util.find_spec(\"tensorflow\") is not None\n",
        "        and version.parse(importlib.metadata.version(\"tensorflow\")) < version.parse(\"2.18.0\")\n",
        "        and version.parse(importlib.metadata.version(\"numpy\")) >= version.parse(\"2.0.0\")\n",
        "    ):\n",
        "        %pip uninstall -q -y tensorflow\n",
        "except importlib.metadata.PackageNotFoundError:\n",
        "    %pip uninstall -q -y tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nCtkOqWV08FH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "if not Path(\"notebook_utils.py\").exists():\n",
        "    r = requests.get(\n",
        "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
        "    )\n",
        "    open(\"notebook_utils.py\", \"w\").write(r.text)\n",
        "\n",
        "if not Path(\"cmd_helper.py\").exists():\n",
        "    r = requests.get(\n",
        "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/cmd_helper.py\",\n",
        "    )\n",
        "    open(\"cmd_helper.py\", \"w\").write(r.text)\n",
        "\n",
        "# Read more about telemetry collection at https://github.com/openvinotoolkit/openvino_notebooks?tab=readme-ov-file#-telemetry\n",
        "from notebook_utils import collect_telemetry\n",
        "\n",
        "collect_telemetry(\"whisper-subtitles-generation.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdr9pn8D08FI"
      },
      "source": [
        "## Instantiate model\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. It maps a sequence of audio spectrogram features to a sequence of text tokens. First, the raw audio inputs are converted to a log-Mel spectrogram by action of the feature extractor. Then, the Transformer encoder encodes the spectrogram to form a sequence of encoder hidden states. Finally, the decoder autoregressively predicts text tokens, conditional on both the previous tokens and the encoder hidden states.\n",
        "\n",
        "You can see the model architecture in the diagram below:\n",
        "\n",
        "![whisper_architecture.svg](https://user-images.githubusercontent.com/29454499/204536571-8f6d8d77-5fbd-4c6d-8e29-14e734837860.svg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ayDD-WW08FI"
      },
      "source": [
        "There are several models of different sizes and capabilities trained by the authors of the model. In this tutorial, we will use the `tiny` model, but the same actions are also applicable to other models from Whisper family."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "67941ab232cc43679fa62195046bd882",
            "edc410a00e694a1f81afb4bb5cebebab",
            "ad3d382224014e06a4ba4d4cc0194825"
          ]
        },
        "id": "-Zl9ZxVW08FI",
        "outputId": "f8d17698-c714-4e47-872b-b90336f74172"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Model:', index=7, options=('openai/whisper-large-v3-turbo', 'openai/whisper-large-v3', '…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67941ab232cc43679fa62195046bd882"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "MODELS = [\n",
        "    \"openai/whisper-large-v3-turbo\",\n",
        "    \"openai/whisper-large-v3\",\n",
        "    \"openai/whisper-large-v2\",\n",
        "    \"openai/whisper-large\",\n",
        "    \"openai/whisper-medium\",\n",
        "    \"openai/whisper-small\",\n",
        "    \"openai/whisper-base\",\n",
        "    \"openai/whisper-tiny\",\n",
        "]\n",
        "\n",
        "model_id = widgets.Dropdown(\n",
        "    options=list(MODELS),\n",
        "    value=\"openai/whisper-tiny\",\n",
        "    description=\"Model:\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "model_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJi5PgB208FJ"
      },
      "source": [
        "### Convert model to OpenVINO Intermediate Representation (IR) format using Optimum-Intel.\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Listed Whisper model are available for downloading via the [HuggingFace hub](https://huggingface.co/openai). We will use optimum-cli interface for exporting it into OpenVINO Intermediate Representation (IR) format.\n",
        "\n",
        "Optimum CLI interface for converting models supports export to OpenVINO (supported starting optimum-intel 1.12 version).\n",
        "General command format:\n",
        "\n",
        "```bash\n",
        "optimum-cli export openvino --model <model_id_or_path> --task <task> <output_dir>\n",
        "```\n",
        "\n",
        "where `--model` argument is model id from HuggingFace Hub or local directory with model (saved using `.save_pretrained` method), `--task ` is one of [supported task](https://huggingface.co/docs/optimum/exporters/task_manager) that exported model should solve. For LLMs it will be `automatic-speech-recognition-with-past`. If model initialization requires to use remote code, `--trust-remote-code` flag additionally should be passed. Full list of supported arguments available via `--help` For more details and examples of usage, please check [optimum documentation](https://huggingface.co/docs/optimum/intel/inference#export).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "A3hjiuw608FJ",
        "outputId": "d9d43504-6d51-490b-cd8a-c269055e5782"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Export command:**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "`optimum-cli export openvino --model openai/whisper-large-v2 whisper-large-v2`"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from cmd_helper import optimum_cli\n",
        "\n",
        "model_dir = model_id.value.split(\"/\")[-1]\n",
        "\n",
        "if not Path(model_dir).exists():\n",
        "    optimum_cli(model_id.value, model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eioDGlIF08FJ"
      },
      "source": [
        "## Prepare inference pipeline\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "The image below illustrates the pipeline of video transcribing using the Whisper model.\n",
        "\n",
        "![whisper_pipeline.png](https://user-images.githubusercontent.com/29454499/204536733-1f4342f7-2328-476a-a431-cb596df69854.png)\n",
        "\n",
        "\n",
        "\n",
        "To simplify user experience we will use [OpenVINO Generate API](https://github.com/openvinotoolkit/openvino.genai/blob/master/samples/python/whisper_speech_recognition/README.md). Firstly we will create pipeline with `WhisperPipeline`. You can construct it straight away from the folder with the converted model. It will automatically load the `model`, `tokenizer`, `detokenizer` and default `generation configuration`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqkUOMTP08FJ"
      },
      "source": [
        "### Select inference device\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "select device from dropdown list for running inference using OpenVINO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4edc715b0ede40168416a4514ddfeae0"
          ]
        },
        "id": "fS3XG1R308FJ",
        "outputId": "33dbed2e-32b5-4fa3-c5ed-eb2352f02f18"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4edc715b0ede40168416a4514ddfeae0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from notebook_utils import device_widget\n",
        "\n",
        "device = device_widget(default=\"CPU\")\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyHpDh9i08FJ"
      },
      "outputs": [],
      "source": [
        "import openvino_genai as ov_genai\n",
        "\n",
        "ov_pipe = ov_genai.WhisperPipeline(str(model_dir), device=device.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbV-bmpR08FJ"
      },
      "source": [
        "## Run video transcription pipeline\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Now, we are ready to start transcription. Let's load the video first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "Rk0roh6Y08FJ",
        "outputId": "ac458678-e82c-42ca-c41b-3f59a5ef09b6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'notebook_utils'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b1d828b9944d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebook_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloaded_video.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebook_utils'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from notebook_utils import download_file\n",
        "\n",
        "output_file = Path(\"downloaded_video.mp4\")\n",
        "\n",
        "if not output_file.exists():\n",
        "    download_file(\n",
        "        \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/video/Sheldon%20Cooper%20Jim%20Parsons%20at%20Intels%20Lab.mp4\",\n",
        "        filename=output_file.name,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW-l0kfq08FJ"
      },
      "source": [
        "Select the task for the model:\n",
        "\n",
        "* **transcribe** - generate audio transcription in the source language (automatically detected).\n",
        "* **translate** - generate audio transcription with translation to English language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "de3b9c7002264661a855fa84c3411de7"
          ]
        },
        "id": "1NSORSSm08FJ",
        "outputId": "a6f02210-bafe-4666-c248-1d1ff7413f81"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de3b9c7002264661a855fa84c3411de7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Select(description='Select task:', index=1, options=('transcribe', 'translate'), value='translate')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "task = widgets.Select(\n",
        "    options=[\"transcribe\", \"translate\"],\n",
        "    value=\"translate\",\n",
        "    description=\"Select task:\",\n",
        "    disabled=False,\n",
        ")\n",
        "task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyJoRdPu08FJ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from moviepy import VideoFileClip\n",
        "except ImportError:\n",
        "    from moviepy.editor import VideoFileClip\n",
        "from transformers.pipelines.audio_utils import ffmpeg_read\n",
        "\n",
        "\n",
        "def get_audio(video_file):\n",
        "    \"\"\"\n",
        "    Extract audio signal from a given video file, then convert it to float,\n",
        "    then mono-channel format and resample it to the expected sample rate\n",
        "\n",
        "    Parameters:\n",
        "        video_file: path to input video file\n",
        "    Returns:\n",
        "      resampled_audio: mono-channel float audio signal with 16000 Hz sample rate\n",
        "                       extracted from video\n",
        "      duration: duration of video fragment in seconds\n",
        "    \"\"\"\n",
        "    input_video = VideoFileClip(str(video_file))\n",
        "    duration = input_video.duration\n",
        "    audio_file = video_file.stem + \".wav\"\n",
        "    input_video.audio.write_audiofile(audio_file, logger=None)\n",
        "    with open(audio_file, \"rb\") as f:\n",
        "        inputs = f.read()\n",
        "    audio = ffmpeg_read(inputs, 16000)\n",
        "    return {\n",
        "        \"raw\": audio,\n",
        "        \"sampling_rate\": 16000,\n",
        "    }, duration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGcCrJUo08FK"
      },
      "source": [
        "Let's run generation method. We will put input data as `np array`. Also we will specify `task` and `return_timestamps=True` options. If task is `translate`, you can place `language` option, for example `<|fr|>` for French or it would be detect automatically. We can set up generation parameters in different ways. We can get default config with `get_generation_config()`, setup parameters and put config directly to `generate()`. It's also possible to specify the needed options just as inputs in the `generate()` method and we will use this way. Then we just run `generate` method and get the output in text format.\n",
        "\n",
        "`generate` method with `return_timestamps` set to `True` will return `chunks`, which contain attributes: `text`, `start_ts` and `end_ts`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOk2uEPQ08FK"
      },
      "outputs": [],
      "source": [
        "inputs, duration = get_audio(output_file)\n",
        "\n",
        "transcription = ov_pipe.generate(inputs[\"raw\"], task=task.value, return_timestamps=True).chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzln6OCW08FK"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def format_timestamp(seconds: float):\n",
        "    \"\"\"\n",
        "    format time in srt-file expected format\n",
        "    \"\"\"\n",
        "    assert seconds >= 0, \"non-negative timestamp expected\"\n",
        "    milliseconds = round(seconds * 1000.0)\n",
        "\n",
        "    hours = milliseconds // 3_600_000\n",
        "    milliseconds -= hours * 3_600_000\n",
        "\n",
        "    minutes = milliseconds // 60_000\n",
        "    milliseconds -= minutes * 60_000\n",
        "\n",
        "    seconds = milliseconds // 1_000\n",
        "    milliseconds -= seconds * 1_000\n",
        "\n",
        "    return (f\"{hours}:\" if hours > 0 else \"00:\") + f\"{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
        "\n",
        "\n",
        "def prepare_srt(transcription, filter_duration=None):\n",
        "    \"\"\"\n",
        "    Format transcription into srt file format\n",
        "    \"\"\"\n",
        "    segment_lines = []\n",
        "    for idx, segment in enumerate(transcription):\n",
        "        timestamp = (segment.start_ts, segment.end_ts)\n",
        "        # for the case where the model could not predict an ending timestamp, which can happen if audio is cut off in the middle of a word.\n",
        "        if segment.end_ts == -1:\n",
        "            timestamp[1] = filter_duration\n",
        "\n",
        "        if filter_duration is not None and (timestamp[0] >= math.floor(filter_duration) or timestamp[1] > math.ceil(filter_duration) + 1):\n",
        "            break\n",
        "        segment_lines.append(str(idx + 1) + \"\\n\")\n",
        "        time_start = format_timestamp(timestamp[0])\n",
        "        time_end = format_timestamp(timestamp[1])\n",
        "        time_str = f\"{time_start} --> {time_end}\\n\"\n",
        "        segment_lines.append(time_str)\n",
        "        segment_lines.append(segment.text + \"\\n\\n\")\n",
        "    return segment_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb0_CUCD08FK"
      },
      "source": [
        "\"The results will be saved in the `downloaded_video.srt` file. SRT is one of the most popular formats for storing subtitles and is compatible with many modern video players. This file can be used to embed transcription into videos during playback or by injecting them directly into video files using `ffmpeg`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTJ37lBc08FK"
      },
      "outputs": [],
      "source": [
        "srt_lines = prepare_srt(transcription, filter_duration=duration)\n",
        "# save transcription\n",
        "with output_file.with_suffix(\".srt\").open(\"w\") as f:\n",
        "    f.writelines(srt_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDzl8sQb08FK"
      },
      "source": [
        "Now let us see the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "b27923d770014dde84ce789a0a6042ec"
          ]
        },
        "id": "ZhE31q8O08FK",
        "outputId": "dd5e7a0f-b42b-42b2-e2c0-4fe6cee8e5c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b27923d770014dde84ce789a0a6042ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Video(value=b'\\x00\\x00\\x00\\x18ftypmp42\\x00\\x00\\x00\\x00isommp42\\x00\\x00Aimoov\\x00\\x00\\x00lmvhd...', height='800…"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "widgets.Video.from_file(output_file, loop=False, width=800, height=800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1r1zeuze08FK",
        "outputId": "c7ec60c7-8b24-4cca-9590-55676387c06c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "00:00:00,000 --> 00:00:05,000\n",
            " Oh, what's that?\n",
            "\n",
            "2\n",
            "00:00:05,000 --> 00:00:08,000\n",
            " Oh, wow.\n",
            "\n",
            "3\n",
            "00:00:08,000 --> 00:00:10,000\n",
            " Hello, humans.\n",
            "\n",
            "4\n",
            "00:00:13,000 --> 00:00:15,000\n",
            " Focus on me.\n",
            "\n",
            "5\n",
            "00:00:15,000 --> 00:00:17,000\n",
            " Focus on the guard.\n",
            "\n",
            "6\n",
            "00:00:17,000 --> 00:00:20,000\n",
            " Don't tell anyone what you're seeing in here.\n",
            "\n",
            "7\n",
            "00:00:22,000 --> 00:00:24,000\n",
            " Have you seen what's in there?\n",
            "\n",
            "8\n",
            "00:00:24,000 --> 00:00:25,000\n",
            " They have intel.\n",
            "\n",
            "9\n",
            "00:00:25,000 --> 00:00:27,000\n",
            " This is where it all changes.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\".join(srt_lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xhj8byro08FK"
      },
      "source": [
        "## Quantization\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "[NNCF](https://github.com/openvinotoolkit/nncf/) enables post-training quantization by adding the quantization layers into the model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers. The framework is designed so that modifications to your original training code are minor.\n",
        "\n",
        "The optimization process contains the following steps:\n",
        "\n",
        "1. Create a calibration dataset for quantization.\n",
        "2. Run `nncf.quantize` to obtain quantized encoder and decoder models.\n",
        "3. Serialize the `INT8` model using `openvino.save_model` function.\n",
        "\n",
        ">**Note**: Quantization is time and memory consuming operation. Running quantization code below may take some time.\n",
        "\n",
        "Please select below whether you would like to run Whisper quantization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "14d51e7bb1104de1862ccb72adf305ce"
          ]
        },
        "id": "qDHMEw3L08FK",
        "outputId": "cb56ae44-8ac4-45b8-8ec9-df914d0f6f96"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14d51e7bb1104de1862ccb72adf305ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Checkbox(value=True, description='Quantization')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_quantize = widgets.Checkbox(\n",
        "    value=True,\n",
        "    description=\"Quantization\",\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "to_quantize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDwVd9KS08FK"
      },
      "outputs": [],
      "source": [
        "# Fetch `skip_kernel_extension` module\n",
        "import requests\n",
        "\n",
        "if not Path(\"skip_kernel_extension.py\").exists():\n",
        "    r = requests.get(\n",
        "        url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/skip_kernel_extension.py\",\n",
        "    )\n",
        "    open(\"skip_kernel_extension.py\", \"w\").write(r.text)\n",
        "\n",
        "ov_quantized_model = None\n",
        "quantized_ov_pipe = None\n",
        "\n",
        "%load_ext skip_kernel_extension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Gpa4QIM08FK"
      },
      "source": [
        "Let's load converted OpenVINO model format using Optimum-Intel to easily quantize it.\n",
        "\n",
        "Optimum Intel can be used to load optimized models from the [Hugging Face Hub](https://huggingface.co/docs/optimum/intel/hf.co/models) or local folder to create pipelines to run an inference with OpenVINO Runtime using Hugging Face APIs. The Optimum Inference models are API compatible with Hugging Face Transformers models. This means we just need to replace the `AutoModelForXxx` class with the corresponding `OVModelForXxx` class.\n",
        "\n",
        "Below is an example of the whisper-tiny model\n",
        "\n",
        "```diff\n",
        "-from transformers import AutoModelForSpeechSeq2Seq\n",
        "+from optimum.intel.openvino import OVModelForSpeechSeq2Seq\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "model_id = \"openai/whisper-tiny\"\n",
        "-model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n",
        "+model = OVModelForSpeechSeq2Seq.from_pretrained(model_id, export=True)\n",
        "```\n",
        "\n",
        "Like the original PyTorch model, the OpenVINO model is also compatible with HuggingFace [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) interface for `automatic-speech-recognition`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV9rQc3d08FK"
      },
      "outputs": [],
      "source": [
        "%%skip not $to_quantize.value\n",
        "\n",
        "from transformers import AutoProcessor\n",
        "from optimum.intel.openvino import OVModelForSpeechSeq2Seq\n",
        "\n",
        "ov_model = OVModelForSpeechSeq2Seq.from_pretrained(model_dir, device=device.value)\n",
        "processor = AutoProcessor.from_pretrained(model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCL0yg7f08FK"
      },
      "source": [
        "### Prepare calibration datasets\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "First step is to prepare calibration datasets for quantization. Since we quantize whisper encoder and decoder separately, we need to prepare a calibration dataset for each of the models. We import an `InferRequestWrapper` class that will intercept model inputs and collect them to a list. Then we run model inference on some small amount of audio samples. Generally, increasing the calibration dataset size improves quantization quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_51hRItS08FK"
      },
      "outputs": [],
      "source": [
        "%%skip not $to_quantize.value\n",
        "\n",
        "from itertools import islice\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import pipeline\n",
        "from optimum.intel.openvino.quantization import InferRequestWrapper\n",
        "import torch\n",
        "import platform\n",
        "\n",
        "additional_args = {}\n",
        "\n",
        "if platform.processor() == \"arm\":\n",
        "    additional_args = {\"device\": torch.device(\"cpu\")}\n",
        "\n",
        "\n",
        "def collect_calibration_dataset(ov_model: OVModelForSpeechSeq2Seq, calibration_dataset_size: int):\n",
        "    # Overwrite model request properties, saving the original ones for restoring later\n",
        "    encoder_calibration_data = []\n",
        "    decoder_calibration_data = []\n",
        "    ov_model.encoder.request = InferRequestWrapper(ov_model.encoder.request, encoder_calibration_data, apply_caching=True)\n",
        "    ov_model.decoder.request = InferRequestWrapper(ov_model.decoder.request,\n",
        "                                                             decoder_calibration_data,\n",
        "                                                             apply_caching=True)\n",
        "\n",
        "    pipe = pipeline(\n",
        "      \"automatic-speech-recognition\",\n",
        "      model=ov_model,\n",
        "      chunk_length_s=30,\n",
        "      tokenizer=processor.tokenizer,\n",
        "      feature_extractor=processor.feature_extractor,\n",
        "      **additional_args\n",
        "    )\n",
        "    try:\n",
        "        calibration_dataset = dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"validation\", streaming=True, trust_remote_code=True)\n",
        "        for sample in tqdm(islice(calibration_dataset, calibration_dataset_size), desc=\"Collecting calibration data\",\n",
        "                           total=calibration_dataset_size):\n",
        "            pipe(sample[\"audio\"], generate_kwargs={\"task\": task.value}, return_timestamps=True)\n",
        "    finally:\n",
        "        ov_model.encoder.request = ov_model.encoder.request.request\n",
        "        ov_model.decoder.request = ov_model.decoder.request.request\n",
        "\n",
        "    return encoder_calibration_data, decoder_calibration_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7y0VXPo08FL"
      },
      "source": [
        "### Quantize Whisper encoder and decoder models\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Below we run the `quantize` function which calls `nncf.quantize` on Whisper encoder and decoder-with-past models. We don't quantize first-step-decoder because its share in whole inference time is negligible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xVUYtAo08FL"
      },
      "outputs": [],
      "source": [
        "%%skip not $to_quantize.value\n",
        "\n",
        "import gc\n",
        "import shutil\n",
        "import nncf\n",
        "import openvino as ov\n",
        "\n",
        "\n",
        "CALIBRATION_DATASET_SIZE = 30\n",
        "quantized_model_path = Path(f\"{model_dir}_quantized\")\n",
        "\n",
        "\n",
        "def quantize(ov_model: OVModelForSpeechSeq2Seq, calibration_dataset_size: int):\n",
        "    if not quantized_model_path.exists():\n",
        "        encoder_calibration_data, decoder_calibration_data = collect_calibration_dataset(ov_model, calibration_dataset_size)\n",
        "        print(\"Quantizing encoder\")\n",
        "        quantized_encoder = nncf.quantize(\n",
        "            ov_model.encoder.model,\n",
        "            nncf.Dataset(encoder_calibration_data),\n",
        "            subset_size=len(encoder_calibration_data),\n",
        "            model_type=nncf.ModelType.TRANSFORMER,\n",
        "            # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
        "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.80),\n",
        "        )\n",
        "        ov.save_model(quantized_encoder, quantized_model_path / \"openvino_encoder_model.xml\")\n",
        "        del quantized_encoder\n",
        "        del encoder_calibration_data\n",
        "        gc.collect()\n",
        "\n",
        "        print(\"Quantizing decoder\")\n",
        "        quantized_decoder = nncf.quantize(\n",
        "            ov_model.decoder.model,\n",
        "            nncf.Dataset(decoder_calibration_data),\n",
        "            subset_size=len(decoder_calibration_data),\n",
        "            model_type=nncf.ModelType.TRANSFORMER,\n",
        "            # Smooth Quant algorithm reduces activation quantization error; optimal alpha value was obtained through grid search\n",
        "            advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.96),\n",
        "        )\n",
        "        ov.save_model(quantized_decoder, quantized_model_path / \"openvino_decoder_model.xml\")\n",
        "        del quantized_decoder\n",
        "        del decoder_calibration_data\n",
        "        gc.collect()\n",
        "\n",
        "        # Copy the config file and the first-step-decoder manually\n",
        "        model_path = Path(model_dir)\n",
        "        shutil.copy(model_path / \"config.json\", quantized_model_path / \"config.json\")\n",
        "        shutil.copy(model_path / \"generation_config.json\", quantized_model_path / \"generation_config.json\")\n",
        "        shutil.copy(model_path / \"openvino_decoder_model.xml\", quantized_model_path / \"openvino_decoder_model.xml\")\n",
        "        shutil.copy(model_path / \"openvino_decoder_model.bin\", quantized_model_path / \"openvino_decoder_model.bin\")\n",
        "        shutil.copy(model_path / \"openvino_tokenizer.xml\", quantized_model_path / \"openvino_tokenizer.xml\")\n",
        "        shutil.copy(model_path / \"openvino_tokenizer.bin\", quantized_model_path / \"openvino_tokenizer.bin\")\n",
        "        shutil.copy(model_path / \"openvino_detokenizer.xml\", quantized_model_path / \"openvino_detokenizer.xml\")\n",
        "        shutil.copy(model_path / \"openvino_detokenizer.bin\", quantized_model_path / \"openvino_detokenizer.bin\")\n",
        "        shutil.copy(model_path / \"tokenizer_config.json\", quantized_model_path / \"tokenizer_config.json\")\n",
        "        shutil.copy(model_path / \"tokenizer.json\", quantized_model_path / \"tokenizer.json\")\n",
        "        shutil.copy(model_path / \"vocab.json\", quantized_model_path / \"vocab.json\")\n",
        "        shutil.copy(model_path / \"preprocessor_config.json\", quantized_model_path / \"preprocessor_config.json\")\n",
        "        shutil.copy(model_path / \"special_tokens_map.json\", quantized_model_path / \"special_tokens_map.json\")\n",
        "        shutil.copy(model_path / \"normalizer.json\", quantized_model_path / \"normalizer.json\")\n",
        "        shutil.copy(model_path / \"merges.txt\", quantized_model_path / \"merges.txt\")\n",
        "        shutil.copy(model_path / \"added_tokens.json\", quantized_model_path / \"added_tokens.json\")\n",
        "\n",
        "    quantized_ov_pipe = ov_genai.WhisperPipeline(str(quantized_model_path), device=device.value)\n",
        "    return quantized_ov_pipe\n",
        "\n",
        "\n",
        "quantized_ov_pipe = quantize(ov_model, CALIBRATION_DATASET_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3HYzbFS08FL"
      },
      "source": [
        "### Run quantized model inference\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Let's compare the transcription results for original and quantized models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hl6zp0xe08FL"
      },
      "outputs": [],
      "source": [
        "if ov_quantized_model is not None:\n",
        "    inputs, duration = get_audio(output_file)\n",
        "    transcription = quantized_ov_pipe.generate(inputs[\"raw\"], task=task.value, return_timestamps=True).chunks\n",
        "    srt_lines = prepare_srt(transcription, filter_duration=duration)\n",
        "    print(\"\".join(srt_lines))\n",
        "    widgets.Video.from_file(output_file, loop=False, width=800, height=800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZEswisu08FL"
      },
      "source": [
        "### Compare performance and accuracy of the original and quantized models\n",
        "[back to top ⬆️](#Table-of-contents:)\n",
        "\n",
        "Finally, we compare original and quantized Whisper models from accuracy and performance stand-points.\n",
        "\n",
        "To measure accuracy, we use `1 - WER` as a metric, where WER stands for Word Error Rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d748cc7a3f394026907c0ef3780d58a8",
            "c2377a7e2dd840328913bc77e6f51ca9"
          ]
        },
        "id": "CJN7VL_b08FL",
        "outputId": "362a542d-1da7-48f8-b8d6-88d0690bef2c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d748cc7a3f394026907c0ef3780d58a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring performance and accuracy:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2377a7e2dd840328913bc77e6f51ca9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Measuring performance and accuracy:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Whole pipeline performance speedup: 1.452\n",
            "Whisper transcription word accuracy. Original model: 81.77%. Quantized model: 82.97%.\n",
            "Accuracy drop: -1.20%.\n"
          ]
        }
      ],
      "source": [
        "%%skip not $to_quantize.value\n",
        "\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from jiwer import wer, wer_standardize\n",
        "\n",
        "TEST_DATASET_SIZE = 50\n",
        "\n",
        "def calculate_transcription_time_and_accuracy(ov_model, test_samples):\n",
        "    whole_infer_times = []\n",
        "\n",
        "    ground_truths = []\n",
        "    predictions = []\n",
        "    for data_item in tqdm(test_samples, desc=\"Measuring performance and accuracy\"):\n",
        "        start_time = time.perf_counter()\n",
        "        transcription = ov_model.generate(data_item[\"audio\"][\"array\"], return_timestamps=True)\n",
        "        end_time = time.perf_counter()\n",
        "        whole_infer_times.append(end_time - start_time)\n",
        "\n",
        "        ground_truths.append(data_item[\"text\"])\n",
        "        predictions.append(transcription.texts[0])\n",
        "\n",
        "    word_accuracy = (1 - wer(ground_truths, predictions, reference_transform=wer_standardize,\n",
        "                             hypothesis_transform=wer_standardize)) * 100\n",
        "    mean_whole_infer_time = sum(whole_infer_times)\n",
        "    return word_accuracy, mean_whole_infer_time\n",
        "\n",
        "test_dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"validation\", streaming=True, trust_remote_code=True)\n",
        "test_dataset = test_dataset.shuffle(seed=42).take(TEST_DATASET_SIZE)\n",
        "test_samples = [sample for sample in test_dataset]\n",
        "\n",
        "accuracy_original, times_original = calculate_transcription_time_and_accuracy(ov_pipe, test_samples)\n",
        "accuracy_quantized, times_quantized = calculate_transcription_time_and_accuracy(quantized_ov_pipe, test_samples)\n",
        "print(f\"Whole pipeline performance speedup: {times_original / times_quantized:.3f}\")\n",
        "print(f\"Whisper transcription word accuracy. Original model: {accuracy_original:.2f}%. Quantized model: {accuracy_quantized:.2f}%.\")\n",
        "print(f\"Accuracy drop: {accuracy_original - accuracy_quantized:.2f}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iadZ-JDz08FL"
      },
      "source": [
        "## Interactive demo\n",
        "[back to top ⬆️](#Table-of-contents:)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "test_replace": {
          "    demo.launch(debug=True)": "    demo.launch()",
          "    demo.launch(share=True, debug=True)": "    demo.launch(share=True)"
        },
        "id": "-mxACTlv08FL"
      },
      "outputs": [],
      "source": [
        "def_config = ov_pipe.get_generation_config()\n",
        "\n",
        "\n",
        "def transcribe(video_path, task, use_int8):\n",
        "    data_path = Path(video_path)\n",
        "    inputs, duration = get_audio(data_path)\n",
        "    m_pipe = quantized_ov_pipe if use_int8 else ov_pipe\n",
        "\n",
        "    frame_num = len(inputs[\"raw\"]) / 16000\n",
        "    if frame_num > 30:\n",
        "        config = ov_pipe.get_generation_config()\n",
        "        chink_num = math.ceil(frame_num / 30)\n",
        "        config.max_length = chink_num * def_config.max_length\n",
        "        m_pipe.set_generation_config(config)\n",
        "\n",
        "    transcription = m_pipe.generate(inputs[\"raw\"], task=task.lower(), return_timestamps=True).chunks\n",
        "    srt_lines = prepare_srt(transcription, duration)\n",
        "    with data_path.with_suffix(\".srt\").open(\"w\") as f:\n",
        "        f.writelines(srt_lines)\n",
        "    return [str(data_path), str(data_path.with_suffix(\".srt\"))]\n",
        "\n",
        "\n",
        "if not Path(\"gradio_helper.py\").exists():\n",
        "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/whisper-subtitles-generation/gradio_helper.py\")\n",
        "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
        "\n",
        "from gradio_helper import make_demo\n",
        "\n",
        "demo = make_demo(fn=transcribe, quantized=ov_quantized_model is not None, sample_path=output_file)\n",
        "\n",
        "try:\n",
        "    demo.launch(debug=True)\n",
        "except Exception:\n",
        "    demo.launch(share=True, debug=True)\n",
        "# if you are launching remotely, specify server_name and server_port\n",
        "# demo.launch(server_name='your server name', server_port='server port in int')\n",
        "# Read more in the docs: https://gradio.app/docs/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "openvino_notebooks": {
      "imageUrl": "https://user-images.githubusercontent.com/29454499/204548693-1304ef33-c790-490d-8a8b-d5766acb6254.png",
      "tags": {
        "categories": [
          "Model Demos",
          "AI Trends"
        ],
        "libraries": [],
        "other": [],
        "tasks": [
          "Speech Recognition"
        ]
      }
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67941ab232cc43679fa62195046bd882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "openai/whisper-large-v3-turbo",
              "openai/whisper-large-v3",
              "openai/whisper-large-v2",
              "openai/whisper-large",
              "openai/whisper-medium",
              "openai/whisper-small",
              "openai/whisper-base",
              "openai/whisper-tiny"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 2,
            "layout": "IPY_MODEL_edc410a00e694a1f81afb4bb5cebebab",
            "style": "IPY_MODEL_ad3d382224014e06a4ba4d4cc0194825"
          }
        },
        "edc410a00e694a1f81afb4bb5cebebab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad3d382224014e06a4ba4d4cc0194825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}